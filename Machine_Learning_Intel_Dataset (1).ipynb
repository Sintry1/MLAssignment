{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuafPh2gJams"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r Machine-Learning/\n",
        "!git clone https://github.com/Sintry1/Machine-learning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO0cZzHcJdOl",
        "outputId": "3b54f42a-c009-4843-96e2-bcddd7aedb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'Machine-Learning/': No such file or directory\n",
            "Cloning into 'Machine-learning'...\n",
            "remote: Enumerating objects: 59401, done.\u001b[K\n",
            "remote: Total 59401 (delta 0), reused 0 (delta 0), pack-reused 59401\u001b[K\n",
            "Receiving objects: 100% (59401/59401), 1.31 GiB | 36.32 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "Updating files: 100% (59377/59377), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSet = '/content/Machine-learning/seg_train/seg_train'\n",
        "testSet = '/content/Machine-learning/seg_test/seg_test'"
      ],
      "metadata": {
        "id": "NlgsH_qyJfUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testSet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RsrGa2xYLhBv",
        "outputId": "ed6b8f2b-b6bd-4f95-faf8-51afc9456b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Machine-learning/seg_test/seg_test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ImageDataGenerator(\n",
        "        rescale=1./255, # Change pixel value from 0-255 to 0.0 - 1.0 This applies Feature Scaling to every image, by dividing the pixel values by 255\n",
        "        shear_range=0.2, # Adding a shear_range so that images can be processed from different perspectives\n",
        "        zoom_range=0.2, # How far to randomly zoom in/out of an image\n",
        "        horizontal_flip=True) # This will randomly flip our input images horizontally.\n",
        "\n",
        "training_set = train_data.flow_from_directory(\n",
        "        trainingSet, # Path to folder with images\n",
        "        target_size=(150, 150), # This is how big our images will be when they are fed into the CNN\n",
        "        batch_size=32, # This is how many \"rows\" or images it will go through in each Epoch before backpropogating\n",
        "        class_mode='categorical') # use 'categorical' for > 2 class, 'binary' for two-class problems"
      ],
      "metadata": {
        "id": "-_h1NEeDNRAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de2fb70-b73c-492b-f373-44bafb6d26ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14034 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = train_data.flow_from_directory(\n",
        "        testSet, # Path to folder with images\n",
        "        target_size=(150, 150), # This is how big our images will be when they are fed into the CNN\n",
        "        batch_size=32, # This is how many \"rows\" or images it will go through in each Epoch before backpropogating\n",
        "        class_mode='categorical') # use 'categorical' for > 2 class, 'binary' for two-class problems"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yENMfugloNKG",
        "outputId": "a67508a0-22cf-44c4-aae7-7d7ec7a9570d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential() # instantiate new Sequential object\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=6, activation=\"relu\", input_shape=[150, 150, 3])) #Conv2D needs 3 parameters;\n",
        "# 1: filters, which is the number of feature detectors.\n",
        "# 2: kernel_size, which is the size of feature detectors\n",
        "# 3: activation. Finally there's a 4th we should add, which is input_shape. input_shape itself takes 3 parameters. (height, width, dimensions)\n",
        "# input_shape is only needed on the first layer we add, as it determines the size of the images and the dimenions (3 dimensions = color i.e. RGB, 1 dimension = black & white)\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=6, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2)) # Here we have to add 2 parameters;\n",
        "# 1: pool_size, which refers to the feature detector we use when we apply max pooling\n",
        "# 2: strides, which refers to how large our stride should be. This should usually match the pool_size variable too.\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten()) # No parameters needed here, as it is smart enough to know that flattening should be applied to the previous 3 layers.\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=256, activation=\"relu\")) # add a fully connected layer\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=6, activation=\"sigmoid\")) # We change the activation function to sigmoid here because it is the output layer.\n",
        "# We only need 1 unit because the output is binary (cat or dog/0 or 1)\n",
        "\n",
        "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy']) # This method from the Sequential class library compiles our ANN and needs 3 parameters.\n",
        "# The first parameter is the optimiser, the second is the loss function, and the third is the metric(s) such as accuracy.\n",
        "# adam as an optimizer allows us to perform stochastic gradient descent\n",
        "# If you're doing binary classification (like this) the loss function must always be 'binary_crossentropy'\n",
        "# If we were doing non-binary classification, it would need to be 'categorical_crossentropy'\n",
        "# Since we can select multiple metrics, we have to enter them in [], however we're only looking at accuracy here."
      ],
      "metadata": {
        "id": "H1-lOFpgpJol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earlyStop = tf.keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "ijNFZU-Jp-Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=training_set, validation_data=test_set, epochs=25, callbacks=[earlyStop])"
      ],
      "metadata": {
        "id": "LYQgodt2131-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb0d2ea-894d-4f02-8a68-76e509b01cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "439/439 [==============================] - 136s 273ms/step - loss: 1.1217 - accuracy: 0.5438 - val_loss: 0.9471 - val_accuracy: 0.6183\n",
            "Epoch 2/25\n",
            "439/439 [==============================] - 114s 259ms/step - loss: 0.7827 - accuracy: 0.7033 - val_loss: 0.6906 - val_accuracy: 0.7373\n",
            "Epoch 3/25\n",
            "439/439 [==============================] - 114s 258ms/step - loss: 0.6831 - accuracy: 0.7440 - val_loss: 0.7052 - val_accuracy: 0.7373\n",
            "Epoch 4/25\n",
            "439/439 [==============================] - 112s 255ms/step - loss: 0.6022 - accuracy: 0.7751 - val_loss: 0.6131 - val_accuracy: 0.7670\n",
            "Epoch 5/25\n",
            "439/439 [==============================] - 115s 262ms/step - loss: 0.5497 - accuracy: 0.8003 - val_loss: 0.5277 - val_accuracy: 0.8067\n",
            "Epoch 6/25\n",
            "439/439 [==============================] - 113s 258ms/step - loss: 0.5074 - accuracy: 0.8179 - val_loss: 0.5281 - val_accuracy: 0.8173\n",
            "Epoch 7/25\n",
            "439/439 [==============================] - 114s 260ms/step - loss: 0.4902 - accuracy: 0.8230 - val_loss: 0.6009 - val_accuracy: 0.7743\n",
            "Epoch 8/25\n",
            "439/439 [==============================] - 113s 258ms/step - loss: 0.4497 - accuracy: 0.8355 - val_loss: 0.5295 - val_accuracy: 0.8100\n",
            "Epoch 9/25\n",
            "439/439 [==============================] - 113s 258ms/step - loss: 0.4242 - accuracy: 0.8480 - val_loss: 0.5585 - val_accuracy: 0.8067\n",
            "Epoch 10/25\n",
            "439/439 [==============================] - 114s 259ms/step - loss: 0.4002 - accuracy: 0.8523 - val_loss: 0.5052 - val_accuracy: 0.8173\n",
            "Epoch 11/25\n",
            "439/439 [==============================] - 112s 256ms/step - loss: 0.3905 - accuracy: 0.8580 - val_loss: 0.4936 - val_accuracy: 0.8297\n",
            "Epoch 12/25\n",
            "439/439 [==============================] - 113s 257ms/step - loss: 0.3540 - accuracy: 0.8717 - val_loss: 0.5040 - val_accuracy: 0.8213\n",
            "Epoch 13/25\n",
            "439/439 [==============================] - 114s 260ms/step - loss: 0.3510 - accuracy: 0.8710 - val_loss: 0.4565 - val_accuracy: 0.8443\n",
            "Epoch 14/25\n",
            "439/439 [==============================] - 115s 261ms/step - loss: 0.3348 - accuracy: 0.8764 - val_loss: 0.5403 - val_accuracy: 0.8233\n",
            "Epoch 15/25\n",
            "439/439 [==============================] - 113s 257ms/step - loss: 0.3141 - accuracy: 0.8867 - val_loss: 0.5183 - val_accuracy: 0.8387\n",
            "Epoch 16/25\n",
            "439/439 [==============================] - 112s 256ms/step - loss: 0.3068 - accuracy: 0.8900 - val_loss: 0.5625 - val_accuracy: 0.8240\n",
            "Epoch 17/25\n",
            "439/439 [==============================] - 113s 257ms/step - loss: 0.2764 - accuracy: 0.8972 - val_loss: 0.5622 - val_accuracy: 0.8200\n",
            "Epoch 18/25\n",
            "439/439 [==============================] - 115s 263ms/step - loss: 0.2723 - accuracy: 0.9022 - val_loss: 0.5358 - val_accuracy: 0.8207\n",
            "Epoch 19/25\n",
            "439/439 [==============================] - 119s 272ms/step - loss: 0.2574 - accuracy: 0.9058 - val_loss: 0.5523 - val_accuracy: 0.8297\n",
            "Epoch 20/25\n",
            "439/439 [==============================] - 115s 261ms/step - loss: 0.2481 - accuracy: 0.9096 - val_loss: 0.6337 - val_accuracy: 0.8180\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a64ba0c3940>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}